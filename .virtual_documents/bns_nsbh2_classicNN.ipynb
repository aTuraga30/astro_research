import h5py
import torch 
import numpy as np
import torch.nn as nn 
import torch.optim as optim
from sklearn.model_selection import train_test_split

print("Finished importing")


# Data preprocessing - making sure that each of the files has a 0 or 1 corresponding 
# to whether or not we have a bns or a nsbh or not.

bns = h5py.File('comb1_bns.hdf5','r')
nsb = h5py.File('comb5_nsbh.hdf5','r')

eos = 'APR4'
band = 'mAB_R'

bns_eos_band = bns[eos][band] # shape -> (7522, 400), (len(bns_eos_band), len(bns[eos][band][0]))
nsb_eos_band = nsb[eos][band] # shape -> (439, 400)
time_band = bns[eos]['time'] # shape -> (400,)

bns_label = np.zeros(len(bns_eos_band))
nsb_label = np.ones(len(nsb_eos_band))

all_data = np.concatenate([bns_eos_band, nsb_eos_band], axis=0)
all_data_with_time = np.concatenate([all_data, time_band], axis=0)
all_labels = np.concatenate([bns_label, nsb_label], axis=0)

print_structure = True

# Creating the dataset
with h5py.File('light_curves_without_time.hdf5', 'w') as f:
    f.create_dataset('light_curves', data=all_data, compression="gzip", compression_opts=9)
    
    # Dataset for labels
    f.create_dataset('labels', data=all_labels, compression="gzip", compression_opts=9)
    
    # Add metadata
    f.attrs['description'] = "Light curve data for BNS and NSBH mergers, without time"
    f.attrs['bands'] = ["mAB_band"]
    f.attrs['BNS_class'] = 0  # BNS class label
    f.attrs['NSBH_class'] = 1  # NSBH class label

    f.close()

def print_structure(name, obj):
    # Prints the name and type of each item in the HDF5 file
    print(f"{name}: {type(obj)}")
    if isinstance(obj, h5py.Dataset):  # If it's a dataset, print shape and dtype
        print(f"  - Shape: {obj.shape}")
        print(f"  - Data type: {obj.dtype}")
    elif isinstance(obj, h5py.Group):  # If it's a group, list the contents
        print(f"  - Contains: {list(obj.keys())}")

with h5py.File('light_curves_without_time.hdf5', 'r') as f:
    print("File Structure:")
    f.visititems(print_structure)
    
    # Print file-level attributes
    print("\nFile Attributes:")
    for key, value in f.attrs.items():
        print(f"  {key}: {value}")

    f.close()
    
print()
print("Finished pre-processing the data")


# Data splitting into X and y
bns_time_band = bns[eos]['time']
nsb_time_band = nsb[eos]['time']

eos_band_time_file = 'light_curves_without_time.hdf5'

with h5py.File(eos_band_time_file, 'r') as f:
    light_curves = f['light_curves'][:]  # Shape: (7961, 400)
    labels = f['labels'][:]              # Shape: (7961,)

    f.close()

X_bns = light_curves[0:400]
y_bns = labels[0:400]

X_nsbh = light_curves[7522:7960]
y_nsbh = labels[7522:7960]

X = np.concatenate((X_bns, X_nsbh), axis=0)
y = np.concatenate((y_bns, y_nsbh), axis=0)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_train = y_train.long()

X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
y_test = y_test.long()

print("Finished splitting the data")
print(X_train)


model = nn.Sequential(
    nn.BatchNorm1d(400),
    nn.Linear(400, 64),
    nn.ReLU(),  
    nn.Linear(64, 8), 
    nn.ReLU(),
    nn.Linear(8, 2)
)

#class_counts = torch.bincount(y)
#total_class_counts = class_counts.sum()
#class_weights = total_class_counts / class_counts

#print(class_counts)

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

print("Finished the model setup")


num_epochs = 50
batch_size = 8

for epoch in range(num_epochs):
    # perm = torch.randperm(len(X))
    # X = X[perm]
    # y = y[perm]

    perm = torch.randperm(len(X_train))
    X_train = X_train[perm]
    y_train = y_train[perm]

    epoch_loss = 0
    
    for i in range(0, len(X_train), batch_size):
        X_train_batch = X_train[i:i+batch_size]
        y_train_pred = model(X_train_batch)
        y_train_batch = y_train[i:i+batch_size]
        loss = loss_fn(y_train_pred, y_train_batch)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        
    scheduler.step(epoch_loss)
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(X):.4f}")


with torch.no_grad():
    y_test_pred = model(X_test)

y_test_pred_classes = torch.argmax(y_test_pred, dim=1)

accuracy = (y_test_pred_classes == y_test).float().mean()
print(f"Accuracy: {accuracy.item()}")



